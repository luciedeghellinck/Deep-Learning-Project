{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/luciedeghellinck/Deep-Learning-Project/blob/master/Copy_of_Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw1o8Pyhgrtz"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wCVqL9Bw7DNC"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TensorDataset, DataLoader\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets, transforms\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from src.data import Dataset\n",
    "from src.model import CATEModel\n",
    "from src.training import train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZWcUm0o5MUI"
   },
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmJWgBnI5Pdh"
   },
   "outputs": [],
   "source": [
    "n_hidden_layers = 3\n",
    "dim_hidden_layers = 100\n",
    "alpha = 0.356\n",
    "learning_rate = 4.292 * 10**(-4)\n",
    "batch_size = 256\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nZfLbcw6xH4"
   },
   "source": [
    "**Counter-factual cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6tATVFzUNFU"
   },
   "outputs": [],
   "source": [
    "# Require: a set of candidate CATE predictors M\n",
    "MLAlgorithms = []\n",
    "metaLearners = []\n",
    "\n",
    "# Require: an observational validation dataset V\n",
    "dataset = dataset(csv_file)\n",
    "input_size = dataset.size\n",
    "\n",
    "# Step 1: Train f(X,T) by minimising Eq.12 using V\n",
    "model = CATEModel(input_size, n_hidden_layers, dim_hidden_layers, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kiNwkyJ49VJ"
   },
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "criterion = lossFunction()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) #not sure if model.parameters() works since the model depends on the object Module\n",
    "train_loader = DataLoader(th.cat(dataset.data[:, 0], dataset.data[:, 2]), batch_size = batch_size) #not sure about the first parameter for the dataloaders (I took the X and Y here)\n",
    "test_loader = DataLoader(th.cat(dataset.data[:, 0], dataset.data[:, 2]), batch_size = batch_size)\n",
    "\n",
    "epochs = 100 #They don't give the number of epochs... (this is a random number)\n",
    "\n",
    "#for the moment, the dropout hasn't yet been integrated\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train(train_loader, model, optimizer, criterion)\n",
    "    test_loss, test_acc = test(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It2Jt6inZ_en"
   },
   "outputs": [],
   "source": [
    "# Step 2: Estimate the propensity score (done in step 3 since the propensity is used in the definition of tau)\n",
    "# Step 3: Caculate the plug-in tau of samples in V\n",
    "tau = [] # will be an n dimensional vector with the plug-in tau for each feature vector.\n",
    "features = dataset.data[:,0]\n",
    "treatments = dataset.data[:,1]\n",
    "outcomes = dataset.data[:,2]\n",
    "for datapoint in range(dataset.size): # Iterate over each feature vector from the dataset\n",
    "  f0 = dataset.get_hypothesis(features[datapoint], 0) # Obtain the hypothesis for the case where there is no treatment for this feature vector\n",
    "  f1 = dataset.get_hypothesis(features[datapoint], 1) # Obtain the hypothesis for the case where there is a treatment for this feature vector\n",
    "  regression = propensityRegression(dataset) # Obtain the regression function for the propensity for this feature vector\n",
    "  plugIn = plugInTau(features[datapoint], treatments[datapoint], outcomes[datapoint], f0, f1, regression)\n",
    "  tau.append(plugIn)\n",
    "\n",
    "# Step 4: Estimate the performance of candidate predictors in M based on the performance estimator R and tau.\n",
    "performance = th.empty((5, 5)) # Tensor containing at position [i,j] the performance estimator R relative to the MLAlgorithm i and the metaLearner j\n",
    "for algo in MLAlgorithms: \n",
    "  for learner in metaLearners: \n",
    "    candidate = candidatePredictorTau(features, algo, learner) # n dimensional vector with the candidate tau for each feature vector.\n",
    "    performance[algo, learner] = performanceEstimator(tau, candidate) # Equation 5 I think\n",
    "\n",
    "best = th.argmin(performance)\n",
    "bestAlgo = MLAlgorithms[best // 5]\n",
    "bestLearner = metaLearners[best % 5]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMxy9B6pL0n5lZ48iFD3ROk",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}