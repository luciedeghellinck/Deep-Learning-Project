{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMatQXPUKkz3Z+YQWo17U5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciedeghellinck/Deep-Learning-Project/blob/master/Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "yw1o8Pyhgrtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wCVqL9Bw7DNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Propensity calculations**"
      ],
      "metadata": {
        "id": "O2SKpLyo_zQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def propensityScore(dataset, feature_x): \n",
        "  \"\"\"\n",
        "  Calculates the propensity score e(x) = P(T=1|X=x) given a dataset.\n",
        "\n",
        "  Args: \n",
        "    dataset: torch tensor where each row represents a person, the first \n",
        "      column represents the float feature vector, the second is the 0/1 \n",
        "      treatment type and the third is the float outcome.\n",
        "    feature_x: n dimensional float feature vector whose propensity must be \n",
        "      calculated.\n",
        "  Returns: \n",
        "    The float propensity relative to the given feature vector feature_x.\n",
        "  \"\"\"\n",
        "  sum_true = 0\n",
        "  for person in dataset.data: \n",
        "    if person[0] == feature_x: #if the person defined in feature_x is the same as the one in the dataset\n",
        "      if person[1] == 1: #if this person has been treated\n",
        "        sum_true += 1\n",
        "  propensity = sum_true / dataset.size #probability that a person has been treated for this specific feature_x given the input dataset\n",
        "\n",
        "def propensityRegression(dataset): \n",
        "  \"\"\"\n",
        "  Evaluates a regression function for the propensity given the known propensity scores\n",
        "\n",
        "  Args: \n",
        "    dataset: torch tensor where each row represents a person, the first \n",
        "      column represents the float feature vector, the second is the 0/1 \n",
        "      treatment type and the third is the float outcome.\n",
        "  \n",
        "  Returns: \n",
        "    A function that takes an m dimensional feature vector as input and \n",
        "    estimates its propensity\n",
        "  \"\"\"\n",
        "  #Call propensityScore to use the propensity for each input feature vector\n",
        "\n",
        "def propensityEstimate(regression, new_feature_x): \n",
        "  \"\"\"\n",
        "  Using the function obtained from propensityRegression, the estimate of \n",
        "  the propensity of a new feature vector is calculated.\n",
        "\n",
        "  Args: \n",
        "    regression: regression function that allows the propensity to be estimated.\n",
        "    new_feature_x: n dimensional float feature vector whose propensity \n",
        "    must be approximated.\n",
        "  Returns: \n",
        "    The estimated propensity score of the new input feature vector. \n",
        "  \"\"\"\n",
        "  #Use the function found with propensityRegression to estimate the propensity for a feature vector that doesn't belong to the dataset "
      ],
      "metadata": {
        "id": "Q6iL2ZJZ_cTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IPM calculation**"
      ],
      "metadata": {
        "id": "1vLPtjcCAD9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IPM(dataset, representation_false, representation_true): \n",
        "  #Not sure about how this works... I don't know what is the set of functions G\n",
        "  #part of equation 12 but I don't know where in the code it should appear\n",
        "  \"\"\"\n",
        "  Calculates the IPM distance for two probability functions.\n",
        "\n",
        "  Args: \n",
        "    dataset: torch tensor where each row represents a person, the first \n",
        "      column represents the float feature vector, the second is the 0/1 \n",
        "      treatment type and the third is the float outcome.\n",
        "    representation_false: \n",
        "    representation_true:\n",
        "  Returns: \n",
        "    The IPM distance evaluated on all \n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "5YkpDXWV_PMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight calculation**"
      ],
      "metadata": {
        "id": "vXQjcnf6ATAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weight(dataset, x, t):\n",
        "  \"\"\"\n",
        "  Calculates the weight for a given feature vector and a given treatment type.\n",
        "\n",
        "  Args: \n",
        "   dataset: torch tensor where each row represents a person, the first \n",
        "      column represents the float feature vector, the second is the 0/1 \n",
        "      treatment type and the third is the float outcome.\n",
        "    x: m dimensional float feature vector\n",
        "    t: 0/1 treatment type.\n",
        "  Returns: \n",
        "    The float weight for the feature vector and the treatment type.\n",
        "  \"\"\"\n",
        "  regression = propensityRegression(dataset)\n",
        "  propensity = propensityEstimate(regression, x)\n",
        "  weight = (t * (1 - 2 * propensity) + propensity**2) / (propensity * (1 - propensity))\n",
        "  \n",
        "  return weight\n",
        "\n",
        "def pi(dataset, t):\n",
        "  \"\"\"\n",
        "  Calculates the percentage of a given treatment type for the dataset.\n",
        "\n",
        "  Args: \n",
        "    dataset: torch tensor where each row represents a person, the first \n",
        "      column represents the float feature vector, the second is the 0/1 \n",
        "      treatment type and the third is the float outcome.\n",
        "    t: 0/1 treatment type.\n",
        "\n",
        "  Returns: \n",
        "    The float percentage of the treatment type.\n",
        "  \"\"\"\n",
        "  sum = 0\n",
        "  for treatment in dataset.data[1]: \n",
        "    if treatment == t: \n",
        "      sum += 1\n",
        "  pi = sum / dataset.size\n",
        "\n",
        "  return pi\n",
        "\n",
        "def adaptedWeight(dataset, x, t): \n",
        "  \"\"\"\n",
        "  Calculates the weight for a given feature vector and a given treatment type.\n",
        "\n",
        "  Args: \n",
        "   dataset: torch tensor where each row represents a person, the first \n",
        "      column represents the float feature vector, the second is the 0/1 \n",
        "      treatment type and the third is the float outcome.\n",
        "    x: m dimensional float feature vector\n",
        "    t: 0/1 treatment type.\n",
        "\n",
        "  Returns: \n",
        "    The float adapted weight for the feature vector and the treatment type.\n",
        "  \"\"\"\n",
        "  old_weight = weight(dataset, x, t)\n",
        "  pi_0 = pi(dataset, 0)\n",
        "  pi_1 = pi(dataset, 1)\n",
        "  adapted_weight = old_weight / 2 * (t / pi_1 + (1-t) / pi_0)\n",
        "\n",
        "  return adapted_weight"
      ],
      "metadata": {
        "id": "50Sk4ls9AbF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tau and performance estimator**"
      ],
      "metadata": {
        "id": "4-Eb2yptRYjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plugInTau(x, t, y, f0, f1, regression):\n",
        "  \"\"\"\n",
        "  Calculates the plug in tau for the feature vector.\n",
        "\n",
        "  Args: \n",
        "    x: m dimensional float feature vector.\n",
        "    t: 0/1 treatment type.\n",
        "    y: float outcome.\n",
        "    f0: hypothesis function evaluated at the feature vector when there is no treatment.\n",
        "    f1: hypothesis function evaluated at the feature vector when there is a treatment.\n",
        "    regression: regression function for the propensity.\n",
        "\n",
        "  Returns: \n",
        "    A float representing the plug-in predictor for the datapoint [X,T,Y] given the hypotheses functions f0 and f1 as well as the propensity regression function.\n",
        "  \"\"\"\n",
        "  if fT == 0: \n",
        "    fT = f0\n",
        "  else: \n",
        "    fT = f1\n",
        "\n",
        "  propensity = propensityEstimate(regression, x)\n",
        "    \n",
        "  return (t - propensity) / (propensity (1 - propensity)) * (y - fT(x)) + f1(x) - f0(x)\n",
        "\n",
        "def candidatePredictorTau(x, MLAlgorithm, metalearner): \n",
        "  \"\"\"\n",
        "  Calculates the tau for a given a Machine Learning algorithm, a meta-learner and a feature vector.\n",
        "\n",
        "  Args: \n",
        "    x: m dimensional float feature vector.\n",
        "    MLAlgorithm: Machine-learning algorithm.\n",
        "    metaLearner: metal-learner.\n",
        "  Returns: \n",
        "    A float representing the CATE predictor for the algorithm and the meta-learner for the feature. \n",
        "  \"\"\"\n",
        "  #If-else function that checks the machine learning algorithm and the metalearner type\n",
        "  pass\n",
        "\n",
        "def performanceEstimator(plugIn, candidate):\n",
        "  \"\"\"\n",
        "  Calculates the performance estimator for a set of plug-in and candidate tau.\n",
        "\n",
        "  Args: \n",
        "    plugIn: float plug-in CATE preictor\n",
        "    candidate: float candidate CATE predictor from the Maching learning algorithms and the meta-learners\n",
        "  Returns: \n",
        "    A float representing the performance estimator between two CATE predictors.\n",
        "  \"\"\"\n",
        "  #Equation 5\n",
        "  pass"
      ],
      "metadata": {
        "id": "hV5b1YFJRbe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data set**"
      ],
      "metadata": {
        "id": "U7emOyPoFfe4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5srSsB01ULi"
      },
      "outputs": [],
      "source": [
        " class dataset(object): \n",
        "   \"\"\"\n",
        "   Model selection procedure for causal inference models\n",
        "\n",
        "   Args: \n",
        "    dataset: input file containing the features, treatments and outcomes \n",
        "    (add file type)\n",
        "   \"\"\"\n",
        "   def __init__(self, csv_file):\n",
        "    ##Should I use a super init? I don't understand it's use in the assignements...\n",
        "     \"\"\"\n",
        "        Takes a dataset as input and returns a parsed Torch tensor where each \n",
        "        row {X_i, T_i, Y_i} represents the feature vector, the treatment type \n",
        "        and the outcome for a particular person.\n",
        "     \"\"\"\n",
        "    #  Add Chang's parsing of the dataset\n",
        "\n",
        "    # Each n row represents a person; the first column is the m dimensional float\n",
        "    # feature vector, the second column is the 0/1 treatment type, and the \n",
        "    # third column is the float outcome.\n",
        "    self.data = th.Tensor(self.size, 3)\n",
        "\n",
        "    # Number of persons in the dataset\n",
        "    self.size = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed Forward Network**"
      ],
      "metadata": {
        "id": "zTzHImuQFoRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CATEModel(): \n",
        "  \"\"\"\n",
        "  Feed forward neural network that takes the feature vectors as inputs and \n",
        "  estimates the outcome given the treatment type. \n",
        "\n",
        "  Args: \n",
        "\n",
        "  Returns:\n",
        "    \n",
        "  \"\"\"\n",
        "  # Extract the feature vectors from self.dataset for the input of the \n",
        "  # network. Extract the outcomes and treatement type for the weight\n",
        "  # optimisation backward process. \n",
        "\n",
        "  # Add Lucas' forward and backward passes (I am guessing a similar \n",
        "  # structure to the one defined in the net of Assignement A2.3)\n"
      ],
      "metadata": {
        "id": "5kHHShcLk7rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test and train**"
      ],
      "metadata": {
        "id": "gW82efAY6iSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy and paste from the assignements --> check this \n",
        "#Is this where we have to input equation 5?\n",
        "def train(train_loader, net, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Trains network for one epoch in batches.\n",
        "\n",
        "    Args:\n",
        "        train_loader: Data loader for training set.\n",
        "        net: Neural network model.\n",
        "        optimizer: Optimizer (e.g. SGD).\n",
        "        criterion: Loss function (e.g. cross-entropy loss).\n",
        "    \"\"\"\n",
        "  \n",
        "    avg_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # iterate through batches\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep track of loss and accuracy\n",
        "        avg_loss += loss\n",
        "        _, predicted = th.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return avg_loss/len(train_loader), 100 * correct / total\n",
        "        \n",
        "def test(test_loader, net, criterion):\n",
        "    \"\"\"\n",
        "    Evaluates network in batches.\n",
        "\n",
        "    Args:\n",
        "        test_loader: Data loader for test set.\n",
        "        net: Neural network model.\n",
        "        criterion: Loss function (e.g. cross-entropy loss).\n",
        "    \"\"\"\n",
        "\n",
        "    avg_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Use torch.no_grad to skip gradient calculation, not needed for evaluation\n",
        "    with th.no_grad():\n",
        "        # iterate through batches\n",
        "        for data in test_loader:\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # forward pass\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # keep track of loss and accuracy\n",
        "            avg_loss += loss\n",
        "            _, predicted = th.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return avg_loss/len(test_loader), 100 * correct / total"
      ],
      "metadata": {
        "id": "ntsUzJW36kyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**"
      ],
      "metadata": {
        "id": "hZWcUm0o5MUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden_layers = 3\n",
        "dim_hidden_layers = 100\n",
        "alpha = 0.356\n",
        "learning_rate = 4.292 * 10**(-4)\n",
        "batch_size = 256\n",
        "dropout_rate = 0.2"
      ],
      "metadata": {
        "id": "tmJWgBnI5Pdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Counter-factual cross-validation**"
      ],
      "metadata": {
        "id": "6nZfLbcw6xH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Require: a set of candidate CATE predictors M\n",
        "MLAlgorithms = []\n",
        "metaLearners = []\n",
        "\n",
        "# Require: an observational validation dataset V\n",
        "dataset = dataset(csv_file)\n",
        "input_size = dataset.size\n",
        "\n",
        "# Step 1: Train f(X,T) by minimising Eq.12 using V\n",
        "model = CATEModel(input_size, n_hidden_layers, dim_hidden_layers, alpha)"
      ],
      "metadata": {
        "id": "l6tATVFzUNFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() #not sure about the loss type\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate) #not sure if model.parameters() works since the model depends on the object Module\n",
        "train_loader = DataLoader(th.cat(dataset.data[:, 0], dataset.data[:, 2]), batch_size = batch_size) #not sure about the first parameter for the dataloaders (I took the X and Y here)\n",
        "test_loader = DataLoader(th.cat(dataset.data[:, 0], dataset.data[:, 2]), batch_size = batch_size)\n",
        "\n",
        "epochs = 100 #They don't give the number of epochs... (this is a random number)\n",
        "\n",
        "#for the moment, the dropout hasn't yet been integrated\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train(train_loader, model, optimizer, criterion)\n",
        "    test_loss, test_acc = test(test_loader, model, criterion)"
      ],
      "metadata": {
        "id": "3kiNwkyJ49VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Estimate the propensity score (done in step 3 since the propensity is used in the definition of tau)\n",
        "# Step 3: Caculate the plug-in tau of samples in V\n",
        "tau = [] # will be an n dimensional vector with the plug-in tau for each feature vector.\n",
        "features = dataset.data[:,0]\n",
        "treatments = dataset.data[:,1]\n",
        "outcomes = dataset.data[:,2]\n",
        "for datapoint in range(dataset.size): # Iterate over each feature vector from the dataset\n",
        "  f0 = dataset.get_hypothesis(features[datapoint], 0) # Obtain the hypothesis for the case where there is no treatment for this feature vector\n",
        "  f1 = dataset.get_hypothesis(features[datapoint], 1) # Obtain the hypothesis for the case where there is a treatment for this feature vector\n",
        "  regression = propensityRegression(dataset) # Obtain the regression function for the propensity for this feature vector\n",
        "  plugIn = plugInTau(features[datapoint], treatments[datapoint], outcomes[datapoint], f0, f1, regression)\n",
        "  tau.append(plugIn)\n",
        "\n",
        "# Step 4: Estimate the performance of candidate predictors in M based on the performance estimator R and tau.\n",
        "performance = th.empty((5, 5)) # Tensor containing at position [i,j] the performance estimator R relative to the MLAlgorithm i and the metaLearner j\n",
        "for algo in MLAlgorithms: \n",
        "  for learner in metaLearners: \n",
        "    candidate = candidatePredictorTau(features, algo, learner) # n dimensional vector with the candidate tau for each feature vector.\n",
        "    performance[algo, learner] = performanceEstimator(tau, candidate) # Equation 5 I think\n",
        "\n",
        "best = th.argmax(performance)\n",
        "bestAlgo = MLAlgorithms[best // 5]\n",
        "bestLearner = metaLearners[best % 5]"
      ],
      "metadata": {
        "id": "It2Jt6inZ_en"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}